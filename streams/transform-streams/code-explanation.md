# ğŸ§© Custom Transform Stream â€“ Filtering Large JSON Data

## What Are We Building?

â¡ï¸ We are creating a **custom Transform stream** tailored for our specific use case.

The goal is to **process a very large JSON file efficiently**, without loading the entire file into memory.

---

## ğŸ“ Input Data Details

- The file name is `huge_data.json`
- Location: `utils-testing/streams` folder

If the file does not exist:

```bash
python generate_big_file.py
```

This script generates a large JSON file.

---

## ğŸ“„ JSON Structure & Requirement

- Each JSON object contains a property called `isActive`
- `isActive` can be either `true` or `false`

ğŸ¯ **Requirement:**

- Send only records where `isActive === true`
- Filter out all others

---

## âŒ Why We Cannot Use `.filter()`

- The data **is not an array in memory**
- Streams read data **chunk by chunk**, not all at once
- Even if the original file is an array, streaming breaks it into chunks

Each chunk size depends on `highWaterMark`.

For example, if `highWaterMark = 10`, a single JSON object can be split across **multiple chunks**.

---

## ğŸ§  Example of Chunked JSON Data

A single object may arrive like this:

```text
{"id": 1, "us
ername": "user_1
", "email": "u
ser_1@example.
com", "isActi
ve": fal
se, "creat
edAt": "2025-01-0
1T00:00:01Z"}
```

- Data arrives in **binary format**
- JSON objects are **broken into pieces**

So we must:

- Convert binary â†’ string
- Rebuild the original JSON object
- Then apply filtering logic

---

## ğŸ”„ What Does `_transform()` Do?

â¡ï¸ `_transform()` is called by Node.js **every time a new chunk arrives**.

You can think of it as Node.js saying:

> â€œHere is a piece of data. Do whatever you want with it, then tell me when youâ€™re done.â€

---

## ğŸ§¾ `_transform()` Syntax

```js
_transform(chunk, encoding, callback) {
  // process chunk
  callback();
}
```

âš ï¸ **Important:**

If `callback()` is NOT called:

- âŒ Stream freezes
- âŒ No more data flows

---

## ğŸ§  Problems `_transform()` Solves in _Our_ Case

- Data arrives as **binary chunks**
- Chunks may cut JSON objects in half
- We only want records where `isActive === true`

So `_transform()` must:

- ğŸ” Convert binary â†’ text
- ğŸ§± Rebuild complete JSON objects
- ğŸ” Filter based on `isActive`
- ğŸš€ Send valid output downstream

---

## ğŸ”§ Step-by-Step Logic Inside `_transform()`

### 1ï¸âƒ£ Convert Binary to String

```js
chunk.toString();
```

- Converts binary data into readable text

---

### 2ï¸âƒ£ Accumulate Leftover Data

- The converted string is appended to a buffer (`leftover`)
- This buffer holds incomplete data from previous chunks

---

### 3ï¸âƒ£ Split by New Line

- Data is split using `\n`
- This converts the stream into an **array of lines**

---

### 4ï¸âƒ£ Handle Incomplete JSON

- The **last line is removed** using `pop()`
- Reason: the last line may contain **incomplete JSON**

```js
this.buffer = lines.pop();
```

---

### 5ï¸âƒ£ Parse, Filter, Push

- Loop over remaining lines
- Parse each line as JSON
- Check `isActive === true`
- If true â†’ push downstream

```js
this.push(validData);
```

---

### 6ï¸âƒ£ Signal Completion

- Call `callback()` to let Node.js continue processing

---

## ğŸ§¹ `_flush()` Method

â¡ï¸ `_flush()` is called **once**, when the input stream has ended.

It is used to process any **remaining leftover data**.

---

## â“ Why `_flush()` Is REQUIRED

Recall this step:

```js
this.buffer = lines.pop();
```

- At the end of the file, **no more chunks arrive**
- But `this.buffer` may still contain a **complete JSON object**

âŒ Without `_flush()`:

- The **last valid record will be lost**

---

## ğŸ” What Happens Inside `_flush()`

- Works similar to `_transform()`
- Parses the remaining buffer
- Checks `isActive === true`
- Pushes valid data downstream
- Calls `callback()`

Once `_flush()` finishes, it signals that:

âœ… **The entire transform process is complete**

---

## ğŸ§  Key Takeaways

- Streams never guarantee full objects per chunk
- `_transform()` handles **chunk-level processing**
- `_flush()` handles **end-of-stream leftovers**
- Together, they ensure **zero data loss**

---

âœ… This approach enables **memory-efficient filtering of massive JSON files** using Node.js streams.
